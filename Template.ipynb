{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install dev version of the biomancy\n",
    "!pip install -U git+https://github.com/hse-bioinflab/biomancy.git@dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:55:39.598712345Z",
     "start_time": "2023-05-09T06:55:39.486486144Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 11 07:30:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:4D:00.0 Off |                  Off |\n",
      "|  0%   46C    P8    24W / 460W |     19MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check that NVIDIA GPU is present\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths\n",
    "\n",
    "All paths used for training or inference are declared here. Some of them (like **FASTA** or **LABELS**) will need to be changed to point to the correct files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Inputs\n",
    "ROOT = Path.cwd()\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "FASTA = DATA / \"GRCh38.primary_assembly.genome.fa.gz\"\n",
    "BLACKLIST = DATA / \"GRCh38-blacklist.bed\" # Set to None if it's not available\n",
    "LABELS = DATA / \"Z-DNA.GRCh38.bed\"\n",
    "OMIC = DATA / \"omic\"\n",
    "\n",
    "# Outputs\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for model whole-genome predictions\n",
    "PREDICTION = ARTIFACTS / \"whole-genome-predictions\"\n",
    "PREDICTION.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:37:45.306424820Z",
     "start_time": "2023-05-10T10:37:45.296302131Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking that everything is ok\n",
    "for path in ROOT, DATA, FASTA, LABELS, OMIC, ARTIFACTS, PREDICTION:\n",
    "    assert path.exists(), f\"Path {path} doesn't exist!\"\n",
    "\n",
    "assert BLACKLIST is None or BLACKLIST.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import biomancy as bm\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "# See `Partition the genome` section for more details\n",
    "WINDOW_SIZE = 1024\n",
    "BIN_SIZE = WINDOW_SIZE * 100\n",
    "\n",
    "# Batch size used to train/evaluate the model\n",
    "BATCH_SIZE = {\"train\": 64, \"val\": 128, \"test\": 128}\n",
    "\n",
    "# Data samplers used to train/evaluate the model\n",
    "SAMPLERS = {\n",
    "    \"train\": lambda dataset: bm.data.StratifiedGenomicSampler(dataset, BedTool(LABELS)),\n",
    "    \"val\": lambda dataset: SequentialSampler(dataset),\n",
    "    \"test\": lambda dataset: SequentialSampler(dataset),\n",
    "}\n",
    "\n",
    "# Number of threads used to prepare data batches\n",
    "LOADER_THREADS = cpu_count()\n",
    "\n",
    "# Device (GPU or CPU) for training & inference\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Divide the genome into large bins of **BIN_SIZE** base pairs, each of which is subsequently subdivided into windows of **WINDOW_SIZE** base pairs. All windows in one bin are used either for training, validation, or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:37:47.303014874Z",
     "start_time": "2023-05-10T10:37:47.294958814Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch chromosome lengths from the public UCSC database\n",
    "# chromsizes = bm.data.derive.chromsizes(assembly=\"hg38\")\n",
    "\n",
    "# OR read chromosome lengths form the fasta index (use samtools faidx)\n",
    "chromsizes = bm.data.derive.chromsizes(fasta=FASTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:37:47.698033744Z",
     "start_time": "2023-05-10T10:37:47.688200651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep only primary chromosomes and collect their sizes\n",
    "primary = set([f'chr{ind + 1}' for ind in range(22)] + ['chrX', 'chrY', 'chrM'])\n",
    "chromsizes = {k: v for k, v in chromsizes.items() if k in primary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:37:55.438494195Z",
     "start_time": "2023-05-10T10:37:47.984671980Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pybedtools import BedTool\n",
    "\n",
    "# Exclude poorly assembled regions\n",
    "EXCLUDE = bm.data.derive.ambiguous_sites(FASTA)\n",
    "\n",
    "# Exclude blacklisted regions if available \n",
    "# Details: https://doi.org/10.1038/s41598-019-45839-z\n",
    "if BLACKLIST and BLACKLIST.is_file():\n",
    "    EXCLUDE = EXCLUDE.cat(\n",
    "        BedTool(BLACKLIST).sort(), postmerge=True\n",
    "    ).sort()\n",
    "\n",
    "# Keep only 'selected' chromosomes\n",
    "EXCLUDE = EXCLUDE.filter(lambda it: it.chrom in chromsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:37:56.684358539Z",
     "start_time": "2023-05-10T10:37:55.445711257Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pybedtools import Interval\n",
    "\n",
    "# Make bins & take care of the excluded regions\n",
    "chromosomes = [Interval(chrom, 0, length, strand='+') for chrom, length in chromsizes.items()]\n",
    "allgroups = bm.data.derive.bins(chromosomes, binsize=BIN_SIZE, exclude=EXCLUDE, dropshort=False)\n",
    "\n",
    "# Partition into train-val-test bins\n",
    "BINS = {}\n",
    "trainval, BINS['test'] = bm.data.partition.randomly(allgroups, random_state=42)\n",
    "BINS['train'], BINS['val'] = bm.data.partition.randomly(trainval, random_state=34)\n",
    "\n",
    "# Split bins into final windows\n",
    "WINDOWS = {}\n",
    "for key, group in BINS.items():\n",
    "    WINDOWS[key] = bm.data.derive.bins(group, binsize=WINDOW_SIZE, dropshort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we need to define all data sources used in the project (bed, fasta and bigwig files are supported). Each source defined here will be available in batches feed to the model with exactly the same keys as defined here. \n",
    "\n",
    "In this example we define the following sources:\n",
    "\n",
    "- `targets`: target genome annotation in the BED format\n",
    "- `ohe-sequence`: one-hot encoded genome sequence from the FASTA file\n",
    "- `omics`: omics genome annotation in the BED format\n",
    "\n",
    "\n",
    "Output shapes:\n",
    "- `targets`: \\[window size\\]\n",
    "- `ohe-sequence`: \\[4, window size\\]\n",
    "- `omics`: \\[files, window size\\]\n",
    "\n",
    "Note that omics genome annotations can be loaded from the [chip atlas](https://chip-atlas.org/) using the **data.load** module or manually from the [ENCODE](https://www.encodeproject.org/chip-seq-matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:15.079703315Z",
     "start_time": "2023-05-10T10:38:04.804798266Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOURCES = {\n",
    "    'targets': bm.data.sources.BED(LABELS, strand_specific=False, dtype='int64'),\n",
    "    'ohe-sequence': bm.data.sources.fasta.OneHotEncoder(FASTA),\n",
    "    'omics': bm.data.sources.concatenate(\n",
    "        *[bm.data.sources.BED(path, strand_specific=False) for path in OMIC.glob('*.bed.gz')]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, combine the derived bins and data sources to get the final train/val/test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "DATASETS = {key: bm.GenomicDataset(SOURCES, windows) for key, windows in WINDOWS.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervals augmentation\n",
    "\n",
    "To artificially increase the size of the training datasets, we can apply the following transformations to our windows on the fly:\n",
    "\n",
    "- `Shift`: move them a little (here by 50% of the window length)\n",
    "- `RandomizeStrand` and randomize their strand\n",
    "\n",
    "The probability of these transformations is controlled by the **pb** parameter. Can be \"always\", \"never\", or a number within the interval \\[0, 1\\].\n",
    "\n",
    "We use `InjectLimits` to ensure that windows don't go beyond the parent bint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:19.155897324Z",
     "start_time": "2023-05-10T10:38:19.122450485Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS['train'].interval_transform = bm.T.Chain([\n",
    "    bm.T.intervals.InjectLimits(rois=BINS['train']),\n",
    "    bm.T.intervals.Shift(maxshift=0.5, pb='always'),\n",
    "    bm.T.intervals.RandomizeStrand(pb='always')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loaders\n",
    "\n",
    "[Data loaders](https://pytorch.org/docs/stable/data.html) are used to load data in parallel using multiple threads in the order specified by the [samplers](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:33.329233341Z",
     "start_time": "2023-05-10T10:38:26.333147127Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "LOADERS = {\n",
    "    key: DataLoader(dataset, sampler=SAMPLERS[key](dataset), batch_size=BATCH_SIZE[key], num_workers=LOADER_THREADS)\n",
    "    for key, dataset in DATASETS.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:33.412975226Z",
     "start_time": "2023-05-10T10:38:33.361874586Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\t->\t133 batches\n",
      "train\t->\t592 batches\n",
      "val\t->\t99 batches\n"
     ]
    }
   ],
   "source": [
    "# Print how many batches we have for each partition\n",
    "for key, loader in LOADERS.items():\n",
    "    print(f'{key}\\t->\\t{len(loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we need to define the model whose parameters will be optimized during training. The model can include any differentiable functions, also known as layers, that are smooth and have a well-defined gradient. The model class must also inherit from the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) class to enable correct gradient calculations and tracing. You can find more information about available operations in the official [manual](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:37.501050179Z",
     "start_time": "2023-05-10T10:38:37.483234242Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_features, 12, 5, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(12, 12, 5, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(12, 2, 5, padding='same')\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inputs = torch.cat((batch['ohe-sequence'], batch['omics']), 1)\n",
    "        logits = self.layers(inputs)\n",
    "        proba = nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        return {'proba': proba, 'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:38.396295649Z",
     "start_time": "2023-05-10T10:38:38.371431188Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 6\n"
     ]
    }
   ],
   "source": [
    "features = 4 + len(SOURCES['omics'].sources)\n",
    "print('Total features:', features)\n",
    "\n",
    "# Create the model and move to the target device (GPU)\n",
    "MODEL = CNNModel(features).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is a differentiable measure of how well the network is performing on a given task. Ideally, the loss is equal to the target metric, but that's not always possible because many metrics are not differentiable. The loss must produce a single value, which is then used to calculate gradients for all parameters (weights) of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:39.908526372Z",
     "start_time": "2023-05-10T10:38:39.887412591Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        loss = cross_entropy(logits, targets)\n",
    "        return loss\n",
    "\n",
    "\n",
    "LOSS = Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optimizer](https://pytorch.org/docs/stable/optim.html)\n",
    "An optimizer is an algorithm that specifies how gradients will be used to adjust network parameters during training. It's normally called after each batch of data and applies gradients to all (or a subset) of network parameters, which theoretically should lead to minimizing the loss (the gradient's property).\n",
    "\n",
    "Adam is a good default for many problems, you can read about it more [here](https://doi.org/10.48550/arXiv.1412.6980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:38:41.494865701Z",
     "start_time": "2023-05-10T10:38:41.468662590Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Engines](https://pytorch-ignite.ai/concepts/01-engine/)\n",
    "\n",
    "Engine is an [ignite](https://pytorch-ignite.ai/) abstraction that loops a given number of times over provided data, executes a processing function and returns a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "\n",
    "\n",
    "def train_step(engine: Engine, batch):\n",
    "    OPTIMIZER.zero_grad()\n",
    "\n",
    "    # Move all inputs to the target device\n",
    "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "    \n",
    "    # Calculate the loss\n",
    "    outputs = MODEL(batch)\n",
    "    loss = LOSS(logits=outputs['logits'], targets=batch['targets'])\n",
    "    \n",
    "    # Calculate & apply gradients\n",
    "    loss.backward()\n",
    "    OPTIMIZER.step()\n",
    "\n",
    "    return {\"loss\": loss, \"targets\": batch[\"targets\"], \"proba\": outputs[\"proba\"]}\n",
    "\n",
    "\n",
    "# Train engine\n",
    "TRAINER = Engine(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference mode here is used to disable gradients calculation\n",
    "@torch.inference_mode()\n",
    "def eval_step(engine: Engine, batch):\n",
    "    # Move all inputs to the target device\n",
    "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "    \n",
    "    # Calculate outputs & the loss\n",
    "    outputs = MODEL(batch)\n",
    "    loss = LOSS(logits=outputs['logits'], targets=batch['targets'])\n",
    "\n",
    "    return {\"loss\": loss, \"targets\": batch[\"targets\"], \"proba\": outputs[\"proba\"]}\n",
    "\n",
    "\n",
    "# Validation engine\n",
    "VALIDATOR = Engine(eval_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train, we will only report the smoothed loss value and nothing else. The reason is simple: train batches are drawn in a stratified, repetitive manner. So, each positive sample will be observed more than once per epoch. Moreover, each window is augmented before being drawn, so dataset changes a bit from epoch to epoch and results are not comparable.\n",
    "\n",
    "We could run an additional engine to calculate metrics in deterministic non-extended train windows, but this is too computationally intensive and ultimately not very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ignite.metrics import RunningAverage\n",
    "\n",
    "RunningAverage(alpha=0.98, output_transform=lambda r: r['loss']).attach(TRAINER, 'run_avg_train_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, we will focus on metrics that are calculated based on the confusion matrix. Namely - precision, recall, F1, and IOU.\n",
    "\n",
    "Again, due to computational complexity, we will not calculate curve-based metrics such as ROC AUC or PR AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ignite.metrics import IoU, Loss\n",
    "from ignite.metrics.confusion_matrix import ConfusionMatrix, cmPrecision, cmRecall, cmAccuracy\n",
    "\n",
    "# Our target classes\n",
    "CLASSES = ['bg', 'Z-DNA']\n",
    "\n",
    "# Confusion matrix, it will be calculated only once\n",
    "cm = ConfusionMatrix(\n",
    "    num_classes=len(CLASSES),\n",
    "    output_transform=lambda r: {'y_pred': r['proba'], 'y': r['targets']}\n",
    ")\n",
    "\n",
    "# Other metrics will be derived from the parent confusion matrix\n",
    "cmbased = {\n",
    "    \"Precision\": cmPrecision(cm, average=False),\n",
    "    \"Recall\": cmRecall(cm, average=False),\n",
    "    \"IOU\": IoU(cm),\n",
    "}\n",
    "cmbased[\"F1\"] = 2 * cmbased[\"Precision\"] * cmbased[\"Recall\"] / \\\n",
    "                (cmbased[\"Precision\"] + cmbased[\"Recall\"] + 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "METRICS = {}\n",
    "\n",
    "# Slice each metric to get per-cls values\n",
    "for ind, cls in enumerate(CLASSES):\n",
    "    for name, vec in cmbased.items():\n",
    "        METRICS[f\"{name}_{cls}\"] = vec[ind]\n",
    "\n",
    "# Calculate macro-averaged (mean) metric values (ignore 0 cls - bg)\n",
    "for name, vec in cmbased.items():\n",
    "    METRICS[f\"m{name}\"] = vec[1:].mean()\n",
    "\n",
    "METRICS['loss'] = Loss(\n",
    "    output_transform=lambda x: (x['loss'].unsqueeze(0), x['loss'].unsqueeze(0)),\n",
    "    loss_fn=lambda *x: x[0][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to attach validation metrics to the validation engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, metric in METRICS.items():\n",
    "    metric.attach(VALIDATOR, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "For logging metrics and loss values we will use [Tensorboard](https://www.tensorflow.org/tensorboard) (see below on how to view them after the training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x7f2458b093c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.contrib.handlers import TensorboardLogger\n",
    "from ignite.handlers import global_step_from_engine\n",
    "from ignite.engine import Events\n",
    "\n",
    "logger = TensorboardLogger(log_dir=ARTIFACTS / \"logs\")\n",
    "\n",
    "# Log validation metrics\n",
    "logger.attach_output_handler(\n",
    "    VALIDATOR,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"val metrics\",\n",
    "    metric_names=list(METRICS.keys()),\n",
    "    global_step_transform=global_step_from_engine(TRAINER)\n",
    ")\n",
    "\n",
    "# Log train loss\n",
    "logger.attach_output_handler(\n",
    "    TRAINER,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"train loss\",\n",
    "    metric_names=['run_avg_train_loss']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Scheduler](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "A scheduler is an algorithm that manipulates the learning rate (a parameter that scales gradient steps used by the optimizer) to improve the overall efficiency of the network training process. For example, if the learning rate is too high, the network may overshoot the optimal solution and fail to converge, while with a low learning rate, the network may take a long time to converge or get stuck in a suboptimal solution.\n",
    "\n",
    "In this example, the scheduler will start with a high learning rate and then reduce it when no progress is observed for several batches in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x7f24489e9990>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.handlers import LRScheduler, ReduceLROnPlateauScheduler\n",
    "\n",
    "# Either torch scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.LRScheduler(OPTIMIZER, ...)\n",
    "# scheduler = LRScheduler(scheduler)\n",
    "# TRAINER.add_event_handler(Events.EPOCH_COMPLETED, scheduler)\n",
    "\n",
    "# Or ignite-native scheduler\n",
    "scheduler = ReduceLROnPlateauScheduler(\n",
    "    OPTIMIZER, metric_name='run_avg_train_loss',\n",
    "    mode=\"min\", factor=0.35, patience=30, min_lr=1e-5,\n",
    "    threshold_mode='rel', threshold=0.05,\n",
    ")\n",
    "TRAINER.add_event_handler(Events.EPOCH_COMPLETED, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handlers\n",
    "\n",
    "Handlers are called every epoch/iteration and are used to perform various tasks. Here we'll use them to display a progress bar, implement early stopping, and save model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/ignite/contrib/handlers/tqdm_logger.py:127: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x7f2458b09990>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.handlers import EarlyStopping, DiskSaver, Checkpoint\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "# Progress bar\n",
    "ProgressBar(position=0, persist=True).attach(TRAINER, ['run_avg_train_loss'])\n",
    "# ProgressBar(position=0, persist=True).attach(VALIDATOR, [])\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(patience=100, score_function=lambda engine: engine.state.metrics['mF1'], trainer=TRAINER)\n",
    "VALIDATOR.add_event_handler(Events.EPOCH_COMPLETED, es)\n",
    "\n",
    "# Checkpointing\n",
    "saver = DiskSaver(ARTIFACTS / \"checkpoints\", require_empty=False)\n",
    "\n",
    "# - for train\n",
    "to_save = {'model': MODEL, 'optimizer': OPTIMIZER, 'trainer': TRAINER}\n",
    "ckpt = Checkpoint(\n",
    "    to_save, saver, filename_prefix=\"train_\", n_saved=2\n",
    ")\n",
    "TRAINER.add_event_handler(Events.EPOCH_COMPLETED(every=2), ckpt)\n",
    "\n",
    "# - for validation\n",
    "to_save = {'model': MODEL}\n",
    "ckpt = Checkpoint(\n",
    "    to_save,\n",
    "    saver,\n",
    "    filename_prefix=\"best\",\n",
    "    n_saved=1,\n",
    "    score_function=Checkpoint.get_default_score_fn(\"mF1\"),\n",
    "    score_name=\"valid_mF1\"\n",
    ")\n",
    "VALIDATOR.add_event_handler(Events.EPOCH_COMPLETED, ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train -> Val -> Test\n",
    "Finally, we can use created engines to run the training and validation cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0559 [00:03<00:00]\n",
      "Epoch [2/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0403 [00:02<00:00]\n",
      "Epoch [3/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0385 [00:02<00:00]\n",
      "Epoch [4/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0372 [00:02<00:00]\n",
      "Epoch [5/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0369 [00:02<00:00]\n",
      "Epoch [6/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0378 [00:02<00:00]\n",
      "Epoch [7/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0375 [00:02<00:00]\n",
      "Epoch [8/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0363 [00:02<00:00]\n",
      "Epoch [9/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.0365 [00:02<00:00]\n",
      "Epoch [10/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0369 [00:02<00:00]\n",
      "Epoch [11/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0366 [00:02<00:00]\n",
      "Epoch [12/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0362 [00:02<00:00]\n",
      "Epoch [13/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0371 [00:02<00:00]\n",
      "Epoch [14/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0365 [00:02<00:00]\n",
      "Epoch [15/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0362 [00:02<00:00]\n",
      "Epoch [16/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0375 [00:02<00:00]\n",
      "Epoch [17/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0372 [00:02<00:00]\n",
      "Epoch [18/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0368 [00:02<00:00]\n",
      "Epoch [19/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0368 [00:02<00:00]\n",
      "Epoch [20/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0368 [00:02<00:00]\n",
      "Epoch [21/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0367 [00:02<00:00]\n",
      "Epoch [22/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0372 [00:02<00:00]\n",
      "Epoch [23/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [24/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0364 [00:02<00:00]\n",
      "Epoch [25/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0354 [00:02<00:00]\n",
      "Epoch [26/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [27/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0366 [00:02<00:00]\n",
      "Epoch [28/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [29/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0355 [00:02<00:00]\n",
      "Epoch [30/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [31/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0365 [00:02<00:00]\n",
      "Epoch [32/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0362 [00:02<00:00]\n",
      "Epoch [33/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0366 [00:02<00:00]\n",
      "Epoch [34/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0358 [00:02<00:00]\n",
      "Epoch [35/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0359 [00:02<00:00]\n",
      "Epoch [36/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.036 [00:02<00:00]\n",
      "Epoch [37/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0357 [00:02<00:00]\n",
      "Epoch [38/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0355 [00:02<00:00]\n",
      "Epoch [39/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0366 [00:02<00:00]\n",
      "Epoch [40/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [41/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0362 [00:02<00:00]\n",
      "Epoch [42/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0357 [00:02<00:00]\n",
      "Epoch [43/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0359 [00:02<00:00]\n",
      "Epoch [44/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0359 [00:02<00:00]\n",
      "Epoch [45/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0354 [00:02<00:00]\n",
      "Epoch [46/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0363 [00:02<00:00]\n",
      "Epoch [47/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0353 [00:02<00:00]\n",
      "Epoch [48/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0357 [00:02<00:00]\n",
      "Epoch [49/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0357 [00:02<00:00]\n",
      "Epoch [50/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.036 [00:02<00:00]\n",
      "Epoch [51/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0356 [00:02<00:00]\n",
      "Epoch [52/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0353 [00:03<00:00]\n",
      "Epoch [53/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0352 [00:02<00:00]\n",
      "Epoch [54/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0365 [00:02<00:00]\n",
      "Epoch [55/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [56/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0363 [00:02<00:00]\n",
      "Epoch [57/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0353 [00:02<00:00]\n",
      "Epoch [58/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0359 [00:02<00:00]\n",
      "Epoch [59/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0364 [00:02<00:00]\n",
      "Epoch [60/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0358 [00:02<00:00]\n",
      "Epoch [61/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0347 [00:02<00:00]\n",
      "Epoch [62/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0355 [00:02<00:00]\n",
      "Epoch [63/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0352 [00:02<00:00]\n",
      "Epoch [64/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0354 [00:02<00:00]\n",
      "Epoch [65/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0359 [00:02<00:00]\n",
      "Epoch [66/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0357 [00:02<00:00]\n",
      "Epoch [67/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0351 [00:02<00:00]\n",
      "Epoch [68/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0351 [00:02<00:00]\n",
      "Epoch [69/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0359 [00:02<00:00]\n",
      "Epoch [70/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0351 [00:02<00:00]\n",
      "Epoch [71/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0359 [00:02<00:00]\n",
      "Epoch [72/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0358 [00:02<00:00]\n",
      "Epoch [73/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [74/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0356 [00:02<00:00]\n",
      "Epoch [75/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0358 [00:02<00:00]\n",
      "Epoch [76/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0348 [00:02<00:00]\n",
      "Epoch [77/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0362 [00:02<00:00]\n",
      "Epoch [78/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0354 [00:02<00:00]\n",
      "Epoch [79/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0354 [00:02<00:00]\n",
      "Epoch [80/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0366 [00:02<00:00]\n",
      "Epoch [81/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0368 [00:02<00:00]\n",
      "Epoch [82/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0357 [00:02<00:00]\n",
      "Epoch [83/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0345 [00:02<00:00]\n",
      "Epoch [84/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0354 [00:02<00:00]\n",
      "Epoch [85/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0358 [00:02<00:00]\n",
      "Epoch [86/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.035 [00:02<00:00]\n",
      "Epoch [87/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.036 [00:02<00:00]\n",
      "Epoch [88/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0356 [00:02<00:00]\n",
      "Epoch [89/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0361 [00:02<00:00]\n",
      "Epoch [90/100]: [592/592] 100%|██████████████████████████████████████████████████████████████, run_avg_train_loss=0.035 [00:02<00:00]\n",
      "Epoch [91/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0354 [00:02<00:00]\n",
      "Epoch [92/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0358 [00:02<00:00]\n",
      "Epoch [93/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0363 [00:02<00:00]\n",
      "Epoch [94/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0353 [00:02<00:00]\n",
      "Epoch [95/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0351 [00:02<00:00]\n",
      "Epoch [96/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0356 [00:03<00:00]\n",
      "Epoch [97/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0358 [00:02<00:00]\n",
      "Epoch [98/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0355 [00:02<00:00]\n",
      "Epoch [99/100]: [592/592] 100%|█████████████████████████████████████████████████████████████, run_avg_train_loss=0.0347 [00:02<00:00]\n",
      "Epoch [100/100]: [592/592] 100%|████████████████████████████████████████████████████████████, run_avg_train_loss=0.0362 [00:02<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 59200\n",
       "\tepoch: 100\n",
       "\tepoch_length: 592\n",
       "\tmax_epochs: 100\n",
       "\toutput: <class 'dict'>\n",
       "\tbatch: <class 'dict'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation will be evaluated every N train epochs\n",
    "@TRAINER.on(Events.EPOCH_COMPLETED(every=4))\n",
    "def _():\n",
    "    VALIDATOR.run(LOADERS['val'])\n",
    "\n",
    "\n",
    "TRAINER.run(LOADERS['train'], max_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "You can check metrics logged during training/validation with [tensorboard](https://pypi.org/project/tensorboard/). \n",
    "\n",
    "To do this you will need a separate SSH connection with port forwarding:\n",
    "```shell\n",
    "ssh -L localhost:6006:localhost:6006 username@server\n",
    "cd DL-template\n",
    "tensorboard --port 6006 --logdir artifacts/logs\n",
    "```\n",
    "The session will be available [here](http://127.0.0.1:6006)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "We will test the model that showed the best performance on the validation. It should be saved in the **checkpoints** folder, paste its name instead of the ``\"best_model_valid_mF1=XXXXX.pt\"``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "Checkpoint.load_objects(\n",
    "    to_load={\"model\": MODEL},\n",
    "    checkpoint=ARTIFACTS / \"checkpoints\" / \"best_model_valid_mF1=0.2012.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create test-engine\n",
    "TESTER = Engine(eval_step)\n",
    "\n",
    "# Attach required metrics\n",
    "for name, metric in METRICS.items():\n",
    "    metric.attach(TESTER, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "state = TESTER.run(LOADERS['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_bg\t->\t1.000\n",
      "Recall_bg\t->\t0.999\n",
      "IOU_bg\t->\t0.999\n",
      "F1_bg\t->\t0.999\n",
      "Precision_Z-DNA\t->\t0.140\n",
      "Recall_Z-DNA\t->\t0.298\n",
      "IOU_Z-DNA\t->\t0.106\n",
      "F1_Z-DNA\t->\t0.191\n",
      "mPrecision\t->\t0.140\n",
      "mRecall\t->\t0.298\n",
      "mIOU\t->\t0.106\n",
      "mF1\t->\t0.191\n",
      "loss\t->\t0.006\n"
     ]
    }
   ],
   "source": [
    "# Print test metrics\n",
    "for metric, value in state.metrics.items():\n",
    "    print(f\"{metric}\\t->\\t{value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole-genome predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create whole genome predictions, we will use the `serve` module. As with engines, you need to declare a function that receives a batch from the data sources and outputs the probabilities/arrays that you want to store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from biomancy import serve\n",
    "\n",
    "\n",
    "# A function to generate model predictions\n",
    "def predictor(batch):\n",
    "    # Move all features to the target device\n",
    "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "    # Calculate probabilities and transform them to a numpy ndarray\n",
    "    proba = MODEL(batch)['proba'].cpu().numpy()\n",
    "    # You can return any values here\n",
    "    return {\"proba\": proba}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "The strategy outlines how to divide input intervals into windows and handle boundary cases.\n",
    "\n",
    "In this case, we merge input intervals and divide them into overlapping windows. We use only the central 90% of predictions for each bin because predictions near the edges are less accurate. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strategy = serve.strategy.MergeAndBin(WINDOW_SIZE, roisize=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output hooks\n",
    "Output hooks are used to save model predictions in different formats. They are called for each predicted window with the following parameters:\n",
    "\n",
    "* `it`(Interval) - the bin for which predictions were generated  \n",
    "* `roi` (int, int) - part of the input bin (start/end coordinates) where the predictions are reliable  \n",
    "* `inps` (dict) - input features  \n",
    "* `prep` (dict) - values returned by the predictor function  \n",
    "\n",
    "We should use adapters to transform this data into writable records (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigWig\n",
    "* `quantlvl`: quantize output predictions to these levels to save space (i.e., all predictions will be rounded up to these values)\n",
    "* `skip_below`: skip predictions where probability < this threshold.\n",
    "\n",
    "Here, the adapter passes the output of the predictor to the `BigWig.Record` constructor to create write-ready bigwig records. No complicated logic, the adapter is only needed to select the output array we want to store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "quantlvl = np.arange(0.1, 1.1, step=0.1)\n",
    "skip_below = 0.1\n",
    "\n",
    "bw = serve.io.BigWig(\n",
    "    chromsizes, PREDICTION / \"probability.bw\",\n",
    "    adapter=lambda it, roi, _, pred: serve.io.BigWig.Record.from_array(\n",
    "        it, pred['proba'][1, :], roi=roi, quantlvl=quantlvl, skip_below=skip_below\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BED\n",
    "* `thr`: the threshold at which the result is considered positive and will be written to the bed file \n",
    "\n",
    "As above, the adapter is only needed to extract the array of target probabilities from the `predictor` function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thr = 0.75\n",
    "\n",
    "bed = serve.io.BED(\n",
    "    PREDICTION / \"peaks.bed\",\n",
    "    adapter=lambda it, roi, _, pred: serve.io.BED.Record.from_array(it, pred['proba'][1,:], roi=roi, thr=0.75)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Finally, here's how we can run the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.run(\n",
    "    predictor, SOURCES, chromosomes, hooks=(bw, bed), strategy=strategy,\n",
    "    # data loader parameters\n",
    "    num_workers=LOADER_THREADS, batch_size=BATCH_SIZE['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completed, the predictions will be saved in the **PREDICTION** folder. No additional steps are required.\n",
    "\n",
    "\n",
    "Optionally, we can change the read/write permissions of the artifacts folder to allow their full use outside of the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod -R a+rw {ARTIFACTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
